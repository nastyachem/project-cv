import streamlit as st
import cv2
import numpy as np
from PIL import Image, ImageOps
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
import matplotlib.pyplot as plt
from matplotlib import colors
import io
from collections import OrderedDict
from utils import load_image_from_url, create_threshold_slider

st.set_page_config(page_title="Aerial Segmentation", page_icon="üõ∞Ô∏è", layout="wide")

class UNet(nn.Module):
    """U-Net –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    
    def __init__(self, in_channels=3, out_channels=2):
        super(UNet, self).__init__()
        
        # Encoder
        self.enc1 = self.conv_block(in_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)
        
        # Bottleneck
        self.bottleneck = self.conv_block(512, 1024)
        
        # Decoder
        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = self.conv_block(1024, 512)
        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = self.conv_block(512, 256)
        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = self.conv_block(256, 128)
        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = self.conv_block(128, 64)
        
        self.final_conv = nn.Conv2d(64, out_channels, 1)
        
    def conv_block(self, in_channels, out_channels):
        """–ë–ª–æ–∫ –∏–∑ –¥–≤—É—Ö –∫–æ–Ω–≤–æ–ª—é—Ü–∏–π —Å ReLU –∏ BatchNorm"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(F.max_pool2d(enc1, 2))
        enc3 = self.enc3(F.max_pool2d(enc2, 2))
        enc4 = self.enc4(F.max_pool2d(enc3, 2))
        
        # Bottleneck
        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))
        
        # Decoder
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)
        dec4 = self.dec4(dec4)
        
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)
        
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        return self.final_conv(dec1)





@st.cache_resource
def load_unet_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º UNet –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        try:
            # –°–ø–æ—Å–æ–± 1: torch.jit.load
            model = torch.jit.load('weights/aerial_forest_best.pt', map_location=device)
        except Exception as e1:
            try:
                # –°–ø–æ—Å–æ–± 2: torch.load
                model = torch.load('weights/aerial_forest_best.pt', map_location=device)
            except Exception as e2:
                try:
                    # –°–ø–æ—Å–æ–± 3: —Å–æ–∑–¥–∞–µ–º UNet –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
                    model = UNet(in_channels=3, out_channels=2)
                    checkpoint = torch.load('weights/aerial_forest_best.pt', map_location=device)
                    if 'state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['state_dict'])
                    else:
                        model.load_state_dict(checkpoint)
                except Exception as e3:
                    st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª weights/aerial_forest_best.pt")
                    return None, None
        
        model.to(device)
        model.eval()
        return model, device
    except Exception as e:
        st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None, None

def preprocess_image(image, target_size=(512, 512)):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è UNet"""
    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä
    image = image.resize(target_size, Image.Resampling.LANCZOS)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    tensor = transform(image).unsqueeze(0)
    return tensor, image

def denormalize_image(tensor):
    """–î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy
    image = tensor.squeeze().permute(1, 2, 0).cpu().numpy()
    
    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    image = image * std + mean
    image = np.clip(image, 0, 1)
    
    return image

def create_segmentation_overlay(original_image, segmentation_mask, alpha=0.6):
    """–°–æ–∑–¥–∞–µ–º overlay —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
    try:
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–∞—Å–∫–∞ –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
        if segmentation_mask is None:
            st.error("‚ùå –ú–∞—Å–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—É—Å—Ç–∞—è")
            return original_image
            
        # –¶–≤–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤
        colormap = np.array([
            [0, 0, 0],        # Background (—á–µ—Ä–Ω—ã–π)
            [255, 255, 255],  # Forest (–±–µ–ª—ã–π)
        ], dtype=np.uint8)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π –≤ –º–∞—Å–∫–µ
        mask_min, mask_max = segmentation_mask.min(), segmentation_mask.max()
        if mask_max >= len(colormap):
            st.warning(f"‚ö†Ô∏è –ó–Ω–∞—á–µ–Ω–∏—è –≤ –º–∞—Å–∫–µ ({mask_min}-{mask_max}) –ø—Ä–µ–≤—ã—à–∞—é—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ ({len(colormap)})")
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
            segmentation_mask = np.clip(segmentation_mask, 0, len(colormap)-1)
        
        # –°–æ–∑–¥–∞–µ–º —Ü–≤–µ—Ç–Ω—É—é –º–∞—Å–∫—É
        colored_mask = colormap[segmentation_mask]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array
        original_array = np.array(original_image)
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
        if original_array.shape[:2] != colored_mask.shape[:2]:
            # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –º–∞—Å–∫–∏ –ø–æ–¥ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            from PIL import Image as PILImage
            mask_pil = PILImage.fromarray(colored_mask)
            mask_pil = mask_pil.resize((original_array.shape[1], original_array.shape[0]), PILImage.NEAREST)
            colored_mask = np.array(mask_pil)
        
        # –°–æ–∑–¥–∞–µ–º overlay
        overlay = cv2.addWeighted(original_array, 1-alpha, colored_mask, alpha, 0)
        
        return Image.fromarray(overlay.astype(np.uint8))
        
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è overlay: {e}")
        return original_image

def visualize_segmentation_results(original_image, input_tensor, segmentation_mask, model_output):
    """–°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # 1. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    axes[0].imshow(np.array(original_image))
    axes[0].set_title('–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', fontsize=12)
    axes[0].axis('off')
    
    # 2. –ú–∞—Å–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    cmap = colors.ListedColormap(['black', 'white'])
    im = axes[1].imshow(segmentation_mask, cmap=cmap, vmin=0, vmax=1)
    axes[1].set_title('–ú–∞—Å–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏', fontsize=12)
    axes[1].axis('off')
    
    plt.tight_layout()
    return fig

def process_aerial_image(image, model, device, threshold=0.5):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—ç—Ä–æ–∫–æ—Å–º–∏—á–µ—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if model is None:
            return None, None, {}, None, None
            
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        input_tensor, resized_image = preprocess_image(image)
        input_tensor = input_tensor.to(device)
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
        with torch.no_grad():
            output = model(input_tensor)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
            if len(output.shape) == 4:  # [batch, channels, height, width]
                if output.shape[1] == 1:
                    # –û–¥–Ω–æ–∫–∞–Ω–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ (binary segmentation)
                    predictions = torch.sigmoid(output)
                    segmentation_mask = (predictions > threshold).float().cpu().numpy()[0, 0]
                elif output.shape[1] == 2:
                    # –î–≤—É—Ö–∫–∞–Ω–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ (background + forest)
                    predictions = torch.softmax(output, dim=1)
                    segmentation_mask = torch.argmax(predictions, dim=1).cpu().numpy()[0]
                else:
                    # –ú–Ω–æ–≥–æ–∫–∞–Ω–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥
                    predictions = torch.softmax(output, dim=1)
                    segmentation_mask = torch.argmax(predictions, dim=1).cpu().numpy()[0]
            else:
                return None, None, {}, output, input_tensor
            
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω–æ–º—É —Ç–∏–ø—É
            segmentation_mask = segmentation_mask.astype(np.uint8)
            
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ process_aerial_image: {e}")
        return None, None, {}, None, None
    
    # –°–æ–∑–¥–∞–µ–º overlay
    overlay_image = create_segmentation_overlay(resized_image, segmentation_mask)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    unique, counts = np.unique(segmentation_mask, return_counts=True)
    total_pixels = segmentation_mask.size
    
    stats = {}
    class_names = ['Background', 'Forest']
    for i, (class_id, count) in enumerate(zip(unique, counts)):
        if class_id < len(class_names):
            percentage = (count / total_pixels) * 100
            stats[class_names[class_id]] = {
                'pixels': int(count),
                'percentage': round(percentage, 2)
            }
    
    return overlay_image, segmentation_mask, stats, output, input_tensor

def create_segmentation_plot(segmentation_mask):
    """–°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    try:
        if segmentation_mask is None:
            fig, ax = plt.subplots(1, 1, figsize=(8, 6))
            ax.text(0.5, 0.5, '–û—à–∏–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏', ha='center', va='center', fontsize=16)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
            
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        unique_classes = np.unique(segmentation_mask)
        max_class = int(segmentation_mask.max())
        
        # –¶–≤–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤
        if max_class <= 1:
            cmap = colors.ListedColormap(['black', 'white'])
            labels = ['Background', 'Forest']
        else:
            # –î–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤
            cmap = plt.cm.get_cmap('tab10', max_class + 1)
            labels = [f'Class {i}' for i in range(max_class + 1)]
        
        im = ax.imshow(segmentation_mask, cmap=cmap, vmin=0, vmax=max_class)
        ax.set_title('–ö–∞—Ä—Ç–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏')
        ax.axis('off')
        
        # –î–æ–±–∞–≤–ª—è–µ–º colorbar
        if max_class <= 10:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–ª–∞—Å—Å–æ–≤ –Ω–µ –æ—á–µ–Ω—å –º–Ω–æ–≥–æ
            cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
            cbar.set_ticks(list(range(max_class + 1)))
            cbar.set_ticklabels(labels[:max_class + 1])
        
        return fig
        
    except Exception as e:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π –≥—Ä–∞—Ñ–∏–∫ —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        ax.text(0.5, 0.5, f'–û—à–∏–±–∫–∞: {str(e)}', ha='center', va='center', fontsize=12)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        return fig

def main():
    st.title("üõ∞Ô∏è –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∞—ç—Ä–æ–∫–æ—Å–º–∏—á–µ—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤")
    

    
    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    with st.expander("‚ÑπÔ∏è –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏"):
        st.image("graphics/forest.jpeg", use_container_width=True)
        st.markdown("""
        
        - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: U-Net
        - –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: 118MB
        - –ö–ª–∞—Å—Å—ã: Background, Forest
        - –ó–∞–¥–∞—á–∞: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        - –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: 512x512
        """)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model, device = load_unet_model()
    if model is None:
        st.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É weights/aerial_forest_best.pt")
        return
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    threshold = create_threshold_slider(
        "–ü–æ—Ä–æ–≥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", 
        default_value=0.55,
        help_text="–ü–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ –º–∞—Å–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"
    )
    
    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    show_detailed_view = False
    overlay_alpha = 0.6
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
    st.header("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –∞—ç—Ä–æ–∫–æ—Å–º–∏—á–µ—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤")
    
    # –¢–∞–±—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ø–æ—Å–æ–±–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏
    tab1, tab2 = st.tabs(["üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã", "üåê –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ URL"])
    
    with tab1:
        uploaded_files = st.file_uploader(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
            type=['jpg', 'jpeg', 'png', 'tiff'],
            accept_multiple_files=True,
            help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã: JPG, JPEG, PNG, TIFF"
        )
    
    with tab2:
        url_input = st.text_input(
            "–í–≤–µ–¥–∏—Ç–µ URL —Å–ø—É—Ç–Ω–∏–∫–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:",
            placeholder="https://example.com/satellite_image.jpg",
            help="–í–≤–µ–¥–∏—Ç–µ –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –∞—ç—Ä–æ–∫–æ—Å–º–∏—á–µ—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
        )
        
        url_files = None
        if url_input:
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL..."):
                image, error = load_image_from_url(url_input)
                if image:
                    # –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ-—Ñ–∞–π–ª –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–¥–æ–º
                    url_files = [type('MockFile', (), {
                        'name': url_input.split('/')[-1] or 'url_satellite_image.jpg'
                    })]
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ session_state
                    st.session_state.url_satellite_image = image
                    st.success("‚úÖ –°–ø—É—Ç–Ω–∏–∫–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
                else:
                    st.error(f"‚ùå {error}")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    if uploaded_files or url_files:
        if uploaded_files and url_files:
            all_files = uploaded_files + url_files
        elif uploaded_files:
            all_files = uploaded_files
        else:
            all_files = url_files
        uploaded_files = all_files
    
    if uploaded_files:
        st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(uploaded_files)} —Ñ–∞–π–ª(–æ–≤)")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        for idx, uploaded_file in enumerate(uploaded_files):
            st.subheader(f"üõ∞Ô∏è –°–Ω–∏–º–æ–∫ {idx + 1}: {uploaded_file.name}")
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if hasattr(uploaded_file, 'read'):
                    # –û–±—ã—á–Ω—ã–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    image = Image.open(uploaded_file).convert('RGB')
                else:
                    # URL-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ session_state
                    image = st.session_state.url_satellite_image
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞
                with st.spinner("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏..."):
                    overlay_image, segmentation_mask, stats, model_output, input_tensor = process_aerial_image(
                        image, model, device, threshold
                    )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
                if overlay_image is None or segmentation_mask is None:
                    st.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è {uploaded_file.name}")
                    continue
                
                if show_detailed_view:
                    # –î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                    st.markdown("**üîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏:**")
                    detailed_fig = visualize_segmentation_results(
                        image, input_tensor, segmentation_mask, model_output
                    )
                    st.pyplot(detailed_fig)
                    plt.close(detailed_fig)
                else:
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**–û—Ä–∏–≥–∏–Ω–∞–ª:**")
                        st.image(image, use_container_width=True)
                    
                    with col2:
                        st.markdown("**–ö–∞—Ä—Ç–∞ –∫–ª–∞—Å—Å–æ–≤:**")
                        seg_fig = create_segmentation_plot(segmentation_mask)
                        st.pyplot(seg_fig)
                        plt.close(seg_fig)
                

                
                # –ö–Ω–æ–ø–∫–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                col_download1, col_download2 = st.columns(2)
                
                with col_download1:
                    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ overlay
                    img_buffer = io.BytesIO()
                    overlay_image.save(img_buffer, format='PNG')
                    img_buffer.seek(0)
                    
                    st.download_button(
                        label="üíæ –°–∫–∞—á–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é",
                        data=img_buffer.getvalue(),
                        file_name=f"segmented_{uploaded_file.name}",
                        mime="image/png"
                    )
                
                with col_download2:
                    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    if show_detailed_view:
                        detailed_buffer = io.BytesIO()
                        detailed_fig.savefig(detailed_buffer, format='png', dpi=150, bbox_inches='tight')
                        detailed_buffer.seek(0)
                        
                        st.download_button(
                            label="üìä –°–∫–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑",
                            data=detailed_buffer.getvalue(),
                            file_name=f"analysis_{uploaded_file.name.split('.')[0]}.png",
                            mime="image/png"
                        )
                
                st.divider()
                
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {uploaded_file.name}: {e}")
    
    else:
        pass

if __name__ == "__main__":
    main() 